import pandas as pd
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import pad_sequences
from keras.layers import Conv1D, Dense, Embedding, Flatten, MaxPooling1D, Dropout, Bidirectional, LSTM, GlobalMaxPooling1D
from keras.models import Sequential
from sklearn.utils import class_weight
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
from keras.callbacks import EarlyStopping
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split



# -*- coding: utf-8 -*-
"""CNN_bin_ALEXA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dKnki7NwyldxqV3CTMbXlCox8jR0cFjI
"""

# from google.colab import drive
# drive.mount('/content/drive')

# from google.colab import drive
# # monta tu unidad de google drive
# drive.mount('/content/drive')

# # cambiamos el directorio de trabajo a la carpeta donde está almacenado el dataset
# import os
# os.chdir('/content/drive/My Drive/Colab Notebooks/data/alexa/')

ruta = "/Users/fernandoleonfranco/Documents/GitHub/Semestre_III/DataIntelligence/alexa/amazon_alexa.tsv"


# Tengo un unico archivo asi que necesitare tomar del total un porcentaje
data_frame_alexa = pd.read_csv(ruta, sep="\t")
df_train = data_frame_alexa.sample(frac=0.8, random_state=25)
df_test = data_frame_alexa.drop(df_train.index)

print("tamaño training:", df_train.shape)
print("tamaño test:", df_test.shape)

# mostramos los primeros registros del training:
df_train.head()

df_train = df_train.drop(["date"], axis=1)
df_test = df_test.drop(["date"], axis=1)
# renombramos task1 a label
df_train = df_train.rename(columns={"verified_reviews": "text"})
df_test = df_test.rename(columns={"verified_reviews": "text"})
# mostramos los primeros registros del training
df_train.head()
df_test.head()

X_train = df_train["text"].tolist()
y_train = df_train["feedback"].values

X_test = df_test["text"].tolist()
y_test = df_test["feedback"].values

# Mostramos una muestra de los datos
print("Ejemplo de textos de entrenamiento:", X_train[:5])
print("Etiquetas de entrenamiento:", y_train[:5])

# Revisamos si hay valores no válidos en X_train y X_test
print(
    "¿Existen valores no válidos en X_train?",
    any(isinstance(x, float) or pd.isna(x) for x in X_train),
)
print(
    "¿Existen valores no válidos en X_test?",
    any(isinstance(x, float) or pd.isna(x) for x in X_test),
)


# Reemplazamos valores no válidos en X_test
X_test = [str(x) if isinstance(x, str) else "" for x in X_test]

# Confirmamos que ya no hay valores no válidos
print(
    "¿Existen valores no válidos en X_test después de la limpieza?",
    any(isinstance(x, float) or pd.isna(x) for x in X_test),
)



# Iniciamos tokenización
tokenizer = Tokenizer(oov_token=True)
# Entrenamos el tokenizador usando los textos
tokenizer.fit_on_texts(X_train)

# Convertimos los textos a secuencias numericas
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)


# Analizar las longitudes de los textos
length_train = [len(seq) for seq in X_train]
df_lengths = pd.Series(length_train)

# Mostrar estadísticos descriptivos de las longitudes
print(df_lengths.describe(percentiles=[0.25, 0.50, 0.75, 0.90, 0.95, 0.99]))

# Graficar la distribución de las longitudes
df_lengths.hist()


MAX_LEN = 50  # Ajustar según el análisis de longitudes
X_train = pad_sequences(X_train, maxlen=MAX_LEN, padding="post", truncating="post")
X_test = pad_sequences(X_test, maxlen=MAX_LEN, padding="post", truncating="post")

print(f"Tamaño del vocabulario: {len(tokenizer.word_index) + 1}")
print("Ejemplo de secuencia tokenizada de entrenamiento:", X_train[0])

print(f"Forma de X_train: {X_train.shape}")
print(f"Forma de X_test: {X_test.shape}")
print(f"Forma de y_train: {y_train.shape}")
print(f"Forma de y_test: {y_test.shape}")


# ME QUEDE AQUÍ ¿QUÉ SIGUE?

"""## CNN

Una vez que ya hemos procesado el texto, podemos definir el modelo.


"""


# Parámetros
EMBEDDING_SIZE = 50  # Tamaño de los vectores de las palabras
NUM_WORDS = len(tokenizer.word_index) + 1  # Tamaño del vocabulario
MAX_LEN = 50  # Longitud máxima de las secuencias (ya definida)

# Modelo mejorado
model = Sequential([
    Embedding(NUM_WORDS, 100, input_length=MAX_LEN),
    Conv1D(64, 5, activation='relu', kernel_regularizer=l2(0.01)),
    MaxPooling1D(2),
    Dropout(0.2),
    Conv1D(128, 5, activation='relu', kernel_regularizer=l2(0.01)),
    MaxPooling1D(2),
    Dropout(0.2),
    Bidirectional(LSTM(64, return_sequences=True)),
    GlobalMaxPooling1D(),
    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

# Compilar con learning rate ajustado
optimizer = Adam(learning_rate=0.001)
model.compile(loss='binary_crossentropy', 
              optimizer=optimizer,
              metrics=['accuracy'])

# Calcular pesos de clase
class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weights = dict(enumerate(class_weights))

# Dividir X_train y y_train en entrenamiento y validación
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42
)

# Early stopping
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)

# Entrenamiento
history = model.fit(
    X_train,
    y_train,
    epochs=15,
    batch_size=32,
    validation_data=(X_val, y_val),
    class_weight=class_weights,
    callbacks=[early_stopping]
)

# Evaluar al modelo
loss, accuracy = model.evaluate(X_test, y_test, batch_size=16)
print(f"Loss: {loss}, Accuracy: {accuracy}")



# Generar predicciones
y_pred = (model.predict(X_test) > 0.5).astype("int32")

# Reporte de clasificación
print("\nReporte de Clasificación:")
print(classification_report(y_test, y_pred))

# Matriz de confusión
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Matriz de Confusión")
plt.show()



# Graficar Accuracy
plt.plot(history.history["accuracy"], label="Training Accuracy")
plt.plot(history.history["val_accuracy"], label="Validation Accuracy")
plt.legend()
plt.title("Accuracy vs Epochs")
plt.show()

# Graficar Loss
plt.plot(history.history["loss"], label="Training Loss")
plt.plot(history.history["val_loss"], label="Validation Loss")
plt.legend()
plt.title("Loss vs Epochs")
plt.show()

# Evaluación del modelo en test
loss, accuracy = model.evaluate(X_test, y_test, batch_size=16)

# Predicciones del modelo
y_pred = (model.predict(X_test) > 0.5).astype("int32")

# Reporte de clasificación


print("\n*** Resultados del Modelo ***")
print(f"Loss en Test: {loss:.4f}")
print(f"Accuracy en Test: {accuracy:.4f}\n")

# Reporte de clasificación detallado
print("*** Reporte de Clasificación ***")
print(
    classification_report(y_test, y_pred, target_names=["0 (Negativo)", "1 (Positivo)"])
)

# Matriz de confusión
conf_matrix = confusion_matrix(y_test, y_pred)
print("\n*** Matriz de Confusión ***")
print(conf_matrix)

# Curvas del entrenamiento
print("\n*** Curvas de Entrenamiento y Validación ***")
print(f"Última precisión de entrenamiento: {history.history['accuracy'][-1]:.4f}")
print(f"Última precisión de validación: {history.history['val_accuracy'][-1]:.4f}")
print(f"Última pérdida de entrenamiento: {history.history['loss'][-1]:.4f}")
print(f"Última pérdida de validación: {history.history['val_loss'][-1]:.4f}")
